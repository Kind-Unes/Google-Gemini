{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ddc91a3",
   "metadata": {
    "papermill": {
     "duration": 0.008672,
     "end_time": "2024-01-10T01:07:37.524733",
     "exception": false,
     "start_time": "2024-01-10T01:07:37.516061",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Notebook pull command\n",
    "```bash\n",
    "kaggle kernels pull asaniczka/ner-using-gemini -p /media/asaniczka/working/Projects/scrapers/linkedin_job_scraper/kaggle_gemini -m\n",
    "```\n",
    "\n",
    "#### Notebook push commnad\n",
    "```bash\n",
    "kaggle kernels push -p /media/asaniczka/working/Projects/scrapers/linkedin_job_scraper/kaggle_gemini\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2f9107",
   "metadata": {
    "papermill": {
     "duration": 0.007232,
     "end_time": "2024-01-10T01:07:37.540088",
     "exception": false,
     "start_time": "2024-01-10T01:07:37.532856",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 🌟🌟 **BIG STUFF** 🌟🌟\n",
    "\n",
    "This notebook will demonstrate how to use Google's GEMINI as a Named Entity Recognition (NER) Tool. 💫✨\n",
    "\n",
    "The goal is to take a job description like this: 📝\n",
    "\n",
    "```\n",
    "Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best … Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming Experience with dimensional data modeling and schema design in Data Warehouses Familiar with ETL (managing high-quality reliable ETL pipelines) Be familiar with legal compliance (with data management tools) data classification, and retention Location : This role will be located in office twice a week minimum in Palo Alto, San Francisco or Chicago. Salary Range: $180,000 - 250,000 USD base.\n",
    "```\n",
    "\n",
    "and get a list of skills mentioned in the description like this: 🔍\n",
    "\n",
    "```python\n",
    "skills = ['Python', 'Snowflake', 'Airflow', 'ETL', 'Kubernetes', 'Docker', 'Helm', 'Spark', 'pySpark', 'SQL', 'Kafka', 'Storm']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab88e536",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-10T01:07:37.557199Z",
     "iopub.status.busy": "2024-01-10T01:07:37.556539Z",
     "iopub.status.idle": "2024-01-10T01:07:38.772029Z",
     "shell.execute_reply": "2024-01-10T01:07:38.770807Z"
    },
    "papermill": {
     "duration": 1.227104,
     "end_time": "2024-01-10T01:07:38.774792",
     "exception": false,
     "start_time": "2024-01-10T01:07:37.547688",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import datetime\n",
    "import os\n",
    "import time\n",
    "from typing import Optional, Union\n",
    "import json\n",
    "import re\n",
    "import concurrent.futures\n",
    "import csv\n",
    "import subprocess\n",
    "\n",
    "import pandas as pd\n",
    "import pytz\n",
    "\n",
    "import google.generativeai as genai\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "api_key = user_secrets.get_secret(\"GEMINI_API_KEY\")\n",
    "genai.configure(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2896bf12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-10T01:07:38.792600Z",
     "iopub.status.busy": "2024-01-10T01:07:38.792005Z",
     "iopub.status.idle": "2024-01-10T01:07:39.209074Z",
     "shell.execute_reply": "2024-01-10T01:07:39.207822Z"
    },
    "papermill": {
     "duration": 0.429234,
     "end_time": "2024-01-10T01:07:39.211881",
     "exception": false,
     "start_time": "2024-01-10T01:07:38.782647",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "kaggle_apikey = user_secrets.get_secret(\"kaggle_apikey\")\n",
    "kaggle_username = user_secrets.get_secret(\"kaggle_username\")\n",
    "\n",
    "os.environ['KAGGLE_USERNAME'] = kaggle_username\n",
    "os.environ['KAGGLE_KEY'] = kaggle_apikey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e707e5e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-10T01:07:39.228927Z",
     "iopub.status.busy": "2024-01-10T01:07:39.228543Z",
     "iopub.status.idle": "2024-01-10T01:07:39.233197Z",
     "shell.execute_reply": "2024-01-10T01:07:39.232200Z"
    },
    "papermill": {
     "duration": 0.015827,
     "end_time": "2024-01-10T01:07:39.235416",
     "exception": false,
     "start_time": "2024-01-10T01:07:39.219589",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "RAW_CSV_PATH = '/kaggle/input/linkedin-data-engineer-job-postings/postings.csv'\n",
    "PROJECT_NAME = 'linkedin_job_ner'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d0e2f52",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-10T01:07:39.252373Z",
     "iopub.status.busy": "2024-01-10T01:07:39.251682Z",
     "iopub.status.idle": "2024-01-10T01:07:39.256951Z",
     "shell.execute_reply": "2024-01-10T01:07:39.255849Z"
    },
    "papermill": {
     "duration": 0.016195,
     "end_time": "2024-01-10T01:07:39.259171",
     "exception": false,
     "start_time": "2024-01-10T01:07:39.242976",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# GLOBAL VARS\n",
    "LOGGER = None\n",
    "PROJECT_FOLDER = None\n",
    "DATA_FOLDER = None\n",
    "TEMP_FOLDER = None\n",
    "LOG_FOLDER = None\n",
    "LOG_FILE_PATH = None\n",
    "START_TIME = None\n",
    "DATE_TODAY = datetime.datetime.now(pytz.utc).date()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14953845",
   "metadata": {
    "papermill": {
     "duration": 0.007431,
     "end_time": "2024-01-10T01:07:39.274117",
     "exception": false,
     "start_time": "2024-01-10T01:07:39.266686",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Set up a logger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c54dcd3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-10T01:07:39.291472Z",
     "iopub.status.busy": "2024-01-10T01:07:39.290849Z",
     "iopub.status.idle": "2024-01-10T01:07:39.299127Z",
     "shell.execute_reply": "2024-01-10T01:07:39.298288Z"
    },
    "papermill": {
     "duration": 0.020111,
     "end_time": "2024-01-10T01:07:39.301860",
     "exception": false,
     "start_time": "2024-01-10T01:07:39.281749",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def setup_logger() -> None:\n",
    "    \"\"\"\n",
    "    Set up a logger\n",
    "    \"\"\"\n",
    "    # pylint: disable=global-statement\n",
    "    global LOGGER\n",
    "\n",
    "    LOGGER = logging.getLogger(__name__)\n",
    "    LOGGER.setLevel(logging.DEBUG)  # set the logging level to debug\n",
    "\n",
    "    log_format = logging.Formatter(\n",
    "        '%(asctime)s :   %(levelname)s   :   %(message)s')\n",
    "\n",
    "    # init the console logger\n",
    "    stream_handler = logging.StreamHandler()\n",
    "    stream_handler.setLevel(logging.INFO)\n",
    "    stream_handler.setFormatter(log_format)  # add the format\n",
    "    LOGGER.addHandler(stream_handler)\n",
    "\n",
    "    # init the file logger\n",
    "    file_handler = logging.FileHandler(LOG_FILE_PATH)\n",
    "    file_handler.setLevel(logging.DEBUG)\n",
    "    file_handler.setFormatter(log_format)  # add the format\n",
    "    LOGGER.addHandler(file_handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088b7977",
   "metadata": {
    "papermill": {
     "duration": 0.00736,
     "end_time": "2024-01-10T01:07:39.317085",
     "exception": false,
     "start_time": "2024-01-10T01:07:39.309725",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Sets up the project folder and creates log, data, and temp folders.\n",
    "\n",
    "Also creates the path to the log file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2a9c547",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-10T01:07:39.334824Z",
     "iopub.status.busy": "2024-01-10T01:07:39.334445Z",
     "iopub.status.idle": "2024-01-10T01:07:39.342536Z",
     "shell.execute_reply": "2024-01-10T01:07:39.341272Z"
    },
    "papermill": {
     "duration": 0.019605,
     "end_time": "2024-01-10T01:07:39.344759",
     "exception": false,
     "start_time": "2024-01-10T01:07:39.325154",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def setup_basic_file_paths() -> None:\n",
    "    \"\"\"\n",
    "    Sets up the project folder and creates log, data, and temp folders.\n",
    "    Also creates the path to the log file.\n",
    "    \"\"\"\n",
    "    # pylint: disable=global-statement\n",
    "    global PROJECT_FOLDER, DATA_FOLDER, TEMP_FOLDER, LOG_FOLDER, LOG_FILE_PATH\n",
    "\n",
    "    # create the project folder\n",
    "    cwd = os.getcwd()\n",
    "    PROJECT_FOLDER = os.path.join(cwd, PROJECT_NAME)\n",
    "    os.makedirs(PROJECT_FOLDER, exist_ok=True)\n",
    "\n",
    "    # create the data folder\n",
    "    DATA_FOLDER = os.path.join(PROJECT_FOLDER, 'data')\n",
    "    os.makedirs(DATA_FOLDER, exist_ok=True)\n",
    "\n",
    "    # make the temp folder\n",
    "    TEMP_FOLDER = os.path.join(PROJECT_FOLDER, 'temp')\n",
    "    os.makedirs(TEMP_FOLDER, exist_ok=True)\n",
    "\n",
    "    # make the log folder and log file path\n",
    "    LOG_FOLDER = os.path.join(PROJECT_FOLDER, 'logs')\n",
    "    os.makedirs(LOG_FOLDER, exist_ok=True)\n",
    "    LOG_FILE_PATH = os.path.join(LOG_FOLDER, f'{PROJECT_NAME}.log')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28d42da",
   "metadata": {
    "papermill": {
     "duration": 0.007726,
     "end_time": "2024-01-10T01:07:39.360442",
     "exception": false,
     "start_time": "2024-01-10T01:07:39.352716",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Calculates how long the script has been running\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79b888dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-10T01:07:39.377986Z",
     "iopub.status.busy": "2024-01-10T01:07:39.377266Z",
     "iopub.status.idle": "2024-01-10T01:07:39.382032Z",
     "shell.execute_reply": "2024-01-10T01:07:39.381283Z"
    },
    "papermill": {
     "duration": 0.016227,
     "end_time": "2024-01-10T01:07:39.384287",
     "exception": false,
     "start_time": "2024-01-10T01:07:39.368060",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calc_elapsed_time() -> float:\n",
    "    \"\"\"calculates how long the script has been running and return time in seconds\"\"\"\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = float(end_time - START_TIME)\n",
    "\n",
    "    return elapsed_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f85e7e",
   "metadata": {
    "papermill": {
     "duration": 0.007214,
     "end_time": "2024-01-10T01:07:39.399159",
     "exception": false,
     "start_time": "2024-01-10T01:07:39.391945",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Removes newlines from the given error string.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19c90a23",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-10T01:07:39.415982Z",
     "iopub.status.busy": "2024-01-10T01:07:39.415533Z",
     "iopub.status.idle": "2024-01-10T01:07:39.421944Z",
     "shell.execute_reply": "2024-01-10T01:07:39.420758Z"
    },
    "papermill": {
     "duration": 0.017574,
     "end_time": "2024-01-10T01:07:39.424217",
     "exception": false,
     "start_time": "2024-01-10T01:07:39.406643",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def format_error(error: str) -> str:\n",
    "    \"\"\"\n",
    "    Removes newlines from the given error string.\n",
    "    \"\"\"\n",
    "\n",
    "    error_type = str(type(error))\n",
    "    error = str(error).replace('\\n', '')\n",
    "\n",
    "    formatted_error = f'Error Type: {error_type}, Error: {error}'\n",
    "\n",
    "    return formatted_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5723104a",
   "metadata": {
    "papermill": {
     "duration": 0.00716,
     "end_time": "2024-01-10T01:07:39.439136",
     "exception": false,
     "start_time": "2024-01-10T01:07:39.431976",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 🌟 Function responsible for NER via Gemini\n",
    "\n",
    "Here, first I'll setup a conversation type message template where the user instructs GEMINI. 🚀\n",
    "\n",
    "> 💡\n",
    ">\n",
    "> Gemini API has a rolling window ratelimit where you can burst 60 requests in a few seconds, and then you have to sleep until the next quota set is available. ⏰\n",
    "\n",
    "To achieve this, I'll be using 3 threads to simultaneously process job summaries.\n",
    "\n",
    "This will: 💡\n",
    "\n",
    "1. 🚀 Make sure that I'm always hitting the ratelimit and not staying idle for a gemini response \n",
    "2. ⏰ And once the ratelimit is reached, I'll sleep in 10 sec intervals until new quota is available \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa5e84e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-10T01:07:39.455654Z",
     "iopub.status.busy": "2024-01-10T01:07:39.455209Z",
     "iopub.status.idle": "2024-01-10T01:07:39.467731Z",
     "shell.execute_reply": "2024-01-10T01:07:39.466574Z"
    },
    "papermill": {
     "duration": 0.023671,
     "end_time": "2024-01-10T01:07:39.470192",
     "exception": false,
     "start_time": "2024-01-10T01:07:39.446521",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_ner(idx: int, total: int, summary: str) -> tuple[int, str] | None:\n",
    "    \"\"\"Function responsible for NER via Gemini\"\"\"\n",
    "\n",
    "    LOGGER.info(f'Working on {idx+1}/{total}')\n",
    "    if len(summary) < 100:\n",
    "        return None\n",
    "\n",
    "    model = genai.GenerativeModel(model_name='gemini-pro')\n",
    "\n",
    "    input_message = f\"USER: Imagine you're an NER AI model. \\\n",
    "                    You task is to extract techinical skills, frameworks, languages, softwares, and concepts that are found in the given job posting. \\\n",
    "                    You are allowed to change the names of skills and software to be standard and meaningful.\\\n",
    "                    Make a single list. \\\n",
    "                    The goal is so that the users will get a overview of the skills they need to have. \\\n",
    "                    Do not write sentences, only 1-3 word entities. \\\n",
    "                    Format your response as a list.\\n\\\n",
    "                    USER: Here is the posting: ```{summary}```\\n\\\n",
    "                    AI: \"\n",
    "\n",
    "    retries = 0\n",
    "    gemini_message = None\n",
    "    while retries < 10:\n",
    "        try:\n",
    "            gemini_response = model.generate_content(input_message)\n",
    "            gemini_message = gemini_response.text\n",
    "            break\n",
    "        except Exception as error:\n",
    "            if '429' in str(error):\n",
    "                LOGGER.warning(f'ratelimit on {idx}. Sleeping 10')\n",
    "                time.sleep(10)\n",
    "                retries += 1\n",
    "                continue\n",
    "                \n",
    "            LOGGER.error(f'{format_error(error)} on {idx} via model.generate_content.')\n",
    "            break    \n",
    "\n",
    "    if not gemini_message:\n",
    "        return None\n",
    "    \n",
    "    skills = gemini_message.strip().split('\\n')\n",
    "    extracted_skills = []\n",
    "    for skill in skills:\n",
    "        if len(skill) > 3:\n",
    "            skill = skill.replace('-', '').replace(',', '').strip()\n",
    "            extracted_skills.append(skill)\n",
    "\n",
    "    extracted_skills_str = ', '.join(extracted_skills)\n",
    "\n",
    "    return (idx, extracted_skills_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416e2497",
   "metadata": {
    "papermill": {
     "duration": 0.007429,
     "end_time": "2024-01-10T01:07:39.485283",
     "exception": false,
     "start_time": "2024-01-10T01:07:39.477854",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 🎯 Function responsible for handling adding NER to data\n",
    "\n",
    "It takes in a Pandas DataFrame called `original_df` as input and returns the processed DataFrame `complete_df` as output.\n",
    "\n",
    "#### Steps:\n",
    "\n",
    "1. 📝 Create a mini DataFrame called `mini_df` which includes only the columns 'job_link' and 'job_summary'. A column called 'job_skills' is also added and initialized to `None`.\n",
    "2. 🔮 Use the `concurrent.futures.ThreadPoolExecutor` to process the rows of `mini_df` concurrently with a maximum of 3 worker threads.\n",
    "3. ⚡ For each row in `mini_df`, submit a job to the thread executor by calling the function `extract_ner` with the parameters `idx`, `len(mini_df)`, and `row['job_summary']`. The returned `future` objects are stored in the `futures` list.\n",
    "4. 🔄 Iterate through the `futures` list and wait for each job to complete using `concurrent.futures.as_completed`.\n",
    "5. 📝 Extract the result from each completed job and update the 'job_skills' column in `mini_df` for the corresponding row.\n",
    "6. ⚙️ Merge the processed data from `mini_df` with the original DataFrame (`original_df`) on the 'job_link' column. The resulting DataFrame is assigned to `complete_df`.\n",
    "7. 🔁 Return the `complete_df` as the final output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6dce87d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-10T01:07:39.501925Z",
     "iopub.status.busy": "2024-01-10T01:07:39.501511Z",
     "iopub.status.idle": "2024-01-10T01:07:39.506472Z",
     "shell.execute_reply": "2024-01-10T01:07:39.505364Z"
    },
    "papermill": {
     "duration": 0.016043,
     "end_time": "2024-01-10T01:07:39.508818",
     "exception": false,
     "start_time": "2024-01-10T01:07:39.492775",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "mini_df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ffbc5ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-10T01:07:39.526026Z",
     "iopub.status.busy": "2024-01-10T01:07:39.525576Z",
     "iopub.status.idle": "2024-01-10T01:07:39.536082Z",
     "shell.execute_reply": "2024-01-10T01:07:39.534904Z"
    },
    "papermill": {
     "duration": 0.02195,
     "end_time": "2024-01-10T01:07:39.538409",
     "exception": false,
     "start_time": "2024-01-10T01:07:39.516459",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def handle_adding_job_skills(original_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Function responsible for handling jobs\"\"\"\n",
    "    global mini_df\n",
    "\n",
    "    # create a mini df to use as a working df\n",
    "    mini_df = original_df.loc[:, ['job_link', 'job_summary']]\n",
    "    mini_df['job_skills'] = None\n",
    "\n",
    "    futures = []\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=3) as thread_executor:\n",
    "        for idx, row in mini_df.iterrows():\n",
    "            future = thread_executor.submit(\n",
    "                extract_ner, idx, len(mini_df), row['job_summary'])\n",
    "            futures.append(future)\n",
    "\n",
    "        # handle the results of the futures\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            try:\n",
    "                result = future.result()\n",
    "\n",
    "                if result:\n",
    "                    idx, skills = result\n",
    "                    mini_df.loc[idx, 'job_skills'] = skills\n",
    "\n",
    "            except Exception as error:\n",
    "                LOGGER.critical(\n",
    "                    f'Error when handlig a single job: {format_error(error)}')\n",
    "\n",
    "    # merge the mini_df with the original_df\n",
    "    mini_df.drop('job_summary', axis=1, inplace=True)\n",
    "    complete_df = original_df.merge(mini_df, on='job_link', how='left')\n",
    "\n",
    "    return complete_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16f3c8e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-10T01:07:39.556059Z",
     "iopub.status.busy": "2024-01-10T01:07:39.555688Z",
     "iopub.status.idle": "2024-01-10T01:07:39.563689Z",
     "shell.execute_reply": "2024-01-10T01:07:39.562595Z"
    },
    "papermill": {
     "duration": 0.019914,
     "end_time": "2024-01-10T01:07:39.566029",
     "exception": false,
     "start_time": "2024-01-10T01:07:39.546115",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def upload_to_kaggle() -> None:\n",
    "    \"\"\"Upload the dataset to kaggle\"\"\"\n",
    "    \n",
    "    metadata = {\n",
    "        \"id\":\"asaniczka/linkedin-data-engineer-job-postings\"\n",
    "    }\n",
    "    \n",
    "    # save the metadata file of the dataset\n",
    "    metadata_file_path = os.path.join(DATA_FOLDER,'dataset-metadata.json')\n",
    "    with open(metadata_file_path,'w',encoding='utf-8') as metadata_file:\n",
    "        json.dump(metadata,metadata_file)\n",
    "        \n",
    "    # upload our file to kaggle\n",
    "    retries = 0\n",
    "    while retries < 5:\n",
    "        try:\n",
    "            command = f\"kaggle datasets version -p '{DATA_FOLDER}' -m 'added NER via gemini'\"\n",
    "            subprocess.run(command,shell=True,check=True)\n",
    "            break\n",
    "        except Exception as error:\n",
    "            LOGGER.error(f\"Error on upload_to_kaggle(): {error=}\")\n",
    "            time.sleep(5)\n",
    "            retries+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3f8fb5",
   "metadata": {
    "papermill": {
     "duration": 0.007388,
     "end_time": "2024-01-10T01:07:39.581013",
     "exception": false,
     "start_time": "2024-01-10T01:07:39.573625",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 🎯 Main executor for this notebook\n",
    "\n",
    "💾 This will create a log statement for each processed job posting. Since there's 17K job postings, output cell will contains 17K rows\n",
    "\n",
    "⏳ It might look like crap, but when a kaggle notebook is running, TQDM progress bars don't show up on logs. So this is the simple solution I came up with\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "578ac0a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-10T01:07:39.598023Z",
     "iopub.status.busy": "2024-01-10T01:07:39.597617Z",
     "iopub.status.idle": "2024-01-10T01:07:39.604327Z",
     "shell.execute_reply": "2024-01-10T01:07:39.603444Z"
    },
    "papermill": {
     "duration": 0.018116,
     "end_time": "2024-01-10T01:07:39.606640",
     "exception": false,
     "start_time": "2024-01-10T01:07:39.588524",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def executor() -> None:\n",
    "    \"\"\"Main executor for this module\"\"\"\n",
    "    # pylint: disable=global-statement\n",
    "    global START_TIME\n",
    "\n",
    "    START_TIME = time.time()\n",
    "    setup_basic_file_paths()\n",
    "    setup_logger()\n",
    "\n",
    "    df = pd.read_csv(RAW_CSV_PATH)\n",
    "    #sample_df = df.iloc[:150,:]\n",
    "\n",
    "    completed_df = handle_adding_job_skills(df)\n",
    "    \n",
    "    dataset_path = os.path.join(DATA_FOLDER,'postings.csv')\n",
    "    completed_df.to_csv(dataset_path,\n",
    "                        index=False, quoting=csv.QUOTE_ALL)\n",
    "    \n",
    "    upload_to_kaggle()\n",
    "\n",
    "    time_to_complete = calc_elapsed_time()\n",
    "    LOGGER.info(f\"Completed script in {time_to_complete/60:.2f} mins\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f6db7262",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-01-10T01:07:39.623889Z",
     "iopub.status.busy": "2024-01-10T01:07:39.623451Z",
     "iopub.status.idle": "2024-01-10T01:07:39.628563Z",
     "shell.execute_reply": "2024-01-10T01:07:39.627519Z"
    },
    "papermill": {
     "duration": 0.016375,
     "end_time": "2024-01-10T01:07:39.630726",
     "exception": false,
     "start_time": "2024-01-10T01:07:39.614351",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Not running since the data is already extracted\n",
    "should_run = False\n",
    "\n",
    "if should_run:\n",
    "    executor()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 4183553,
     "isSourceIdPinned": true,
     "sourceId": 7232147,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelInstanceId": 4079,
     "sourceId": 5297,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30626,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 6.25163,
   "end_time": "2024-01-10T01:07:40.258980",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-01-10T01:07:34.007350",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
